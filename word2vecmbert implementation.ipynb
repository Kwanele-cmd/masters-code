{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr, pearsonr\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "isiZulu Word Similarity Benchmark Evaluator - mBERT VERSION\n",
    "Adapted for isiZulu word embeddings using multilingual BERT with support for:\n",
    "- Custom isiZulu word similarity datasets\n",
    "- SimLex-999 and WordSim-353 (if translated to isiZulu)\n",
    "- Precision, Recall, Accuracy, and F1 Score metrics\n",
    "- Handling of isiZulu morphology and agglutination\n",
    "- Contextualized embeddings from mBERT\n",
    "\n",
    "Dependencies:\n",
    "    pip install transformers torch scikit-learn numpy scipy\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix without sklearn.\n",
    "    \n",
    "    Returns:\n",
    "        tn, fp, fn, tp\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def accuracy_np(tp, tn, fp, fn):\n",
    "    \"\"\"Compute accuracy.\"\"\"\n",
    "    total = tp + tn + fp + fn\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return (tp + tn) / total\n",
    "\n",
    "\n",
    "def precision_np(tp, fp):\n",
    "    \"\"\"Compute precision.\"\"\"\n",
    "    if (tp + fp) == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def recall_np(tp, fn):\n",
    "    \"\"\"Compute recall.\"\"\"\n",
    "    if (tp + fn) == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def f1_np(precision, recall):\n",
    "    \"\"\"Compute F1 score.\"\"\"\n",
    "    if (precision + recall) == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "class MBertEmbedder:\n",
    "    \"\"\"\n",
    "    Wrapper for mBERT to generate word embeddings.\n",
    "    Handles contextualized embeddings with multiple pooling strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-multilingual-cased', device=None):\n",
    "        \"\"\"\n",
    "        Initialize mBERT model and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            device: 'cuda', 'cpu', or None (auto-detect)\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Loading {model_name}...\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded on {self.device}\")\n",
    "    \n",
    "    def get_word_embedding(self, word, pooling='mean', layer=-1):\n",
    "        \"\"\"\n",
    "        Get contextualized embedding for a single word.\n",
    "        \n",
    "        Args:\n",
    "            word: isiZulu word (string)\n",
    "            pooling: How to pool subword tokens:\n",
    "                    'mean' - Average all subword embeddings (default)\n",
    "                    'first' - Use first subword token (like [CLS] for full word)\n",
    "                    'last' - Use last subword token\n",
    "                    'max' - Max pooling over subword tokens\n",
    "            layer: Which BERT layer to use:\n",
    "                   -1 = last layer (default)\n",
    "                   -2 = second-to-last layer\n",
    "                   0-11 = specific layer\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of shape (768,) - the word embedding\n",
    "        \"\"\"\n",
    "        # Tokenize word\n",
    "        tokens = self.tokenizer.tokenize(word)\n",
    "        \n",
    "        # Add special tokens\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_ids = torch.tensor([token_ids]).to(self.device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states  # Tuple of (layer, batch, seq, hidden)\n",
    "        \n",
    "        # Select layer\n",
    "        layer_embeddings = hidden_states[layer][0]  # Shape: (seq_len, 768)\n",
    "        \n",
    "        # Remove [CLS] and [SEP] to get only word tokens\n",
    "        word_embeddings = layer_embeddings[1:-1]  # Shape: (n_subwords, 768)\n",
    "        \n",
    "        # Pool subword tokens\n",
    "        if pooling == 'mean':\n",
    "            embedding = word_embeddings.mean(dim=0)\n",
    "        elif pooling == 'first':\n",
    "            embedding = word_embeddings[0]\n",
    "        elif pooling == 'last':\n",
    "            embedding = word_embeddings[-1]\n",
    "        elif pooling == 'max':\n",
    "            embedding = word_embeddings.max(dim=0)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "        \n",
    "        return embedding.cpu().numpy()\n",
    "    \n",
    "    def get_similarity(self, word1, word2, pooling='mean', layer=-1):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two words using mBERT embeddings.\n",
    "        \n",
    "        Args:\n",
    "            word1, word2: isiZulu words\n",
    "            pooling: Pooling strategy for subword tokens\n",
    "            layer: Which BERT layer to use\n",
    "            \n",
    "        Returns:\n",
    "            Cosine similarity (float between -1 and 1)\n",
    "        \"\"\"\n",
    "        emb1 = self.get_word_embedding(word1, pooling=pooling, layer=layer)\n",
    "        emb2 = self.get_word_embedding(word2, pooling=pooling, layer=layer)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "\n",
    "class IsiZuluBenchmarkEvaluatorMBERT:\n",
    "    \"\"\"\n",
    "    Enhanced evaluator for isiZulu word embeddings using mBERT.\n",
    "    Includes both correlation metrics and classification metrics.\n",
    "    \n",
    "    Handles isiZulu-specific challenges:\n",
    "    - Agglutinative morphology (prefixes, suffixes)\n",
    "    - Multiple word forms (singular/plural, noun classes)\n",
    "    - Code-switching with English\n",
    "    - Contextualized embeddings with multiple pooling strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, cache_size=1000, pooling='mean', layer=-1):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with mBERT model.\n",
    "        \n",
    "        Args:\n",
    "            model: MBertEmbedder instance or None (will create default)\n",
    "            cache_size: Size of similarity cache (default: 1000)\n",
    "            pooling: Pooling strategy ('mean', 'first', 'last', 'max')\n",
    "            layer: Which BERT layer to use (-1 = last layer)\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            self.model = MBertEmbedder()\n",
    "        else:\n",
    "            self.model = model\n",
    "        \n",
    "        self.cache_size = cache_size\n",
    "        self.similarity_cache = {}\n",
    "        self.pooling = pooling\n",
    "        self.layer = layer\n",
    "        \n",
    "        print(f\"‚úÖ Evaluator initialized with pooling={pooling}, layer={layer}\")\n",
    "        \n",
    "    def get_similarity(self, word1, word2):\n",
    "        \"\"\"\n",
    "        Get cosine similarity between two isiZulu words using mBERT.\n",
    "        Uses cache to avoid redundant computations.\n",
    "        \n",
    "        Handles isiZulu-specific cases:\n",
    "        - Lowercasing for consistency\n",
    "        - Caching for efficiency\n",
    "        \"\"\"\n",
    "        # Normalize words (lowercase for isiZulu)\n",
    "        word1 = word1.lower().strip()\n",
    "        word2 = word2.lower().strip()\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = tuple(sorted([word1, word2]))\n",
    "        if cache_key in self.similarity_cache:\n",
    "            return self.similarity_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Get similarity from mBERT\n",
    "            sim = self.model.get_similarity(word1, word2, \n",
    "                                           pooling=self.pooling, \n",
    "                                           layer=self.layer)\n",
    "            \n",
    "            # Cache the result\n",
    "            if len(self.similarity_cache) < self.cache_size:\n",
    "                self.similarity_cache[cache_key] = sim\n",
    "            \n",
    "            return sim\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error computing similarity for '{word1}' and '{word2}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    def binarize_scores(self, scores, threshold='median'):\n",
    "        \"\"\"\n",
    "        Convert continuous scores to binary labels.\n",
    "        \n",
    "        Args:\n",
    "            scores: Array of continuous scores\n",
    "            threshold: 'median', 'mean', or a specific numeric value\n",
    "            \n",
    "        Returns:\n",
    "            Binary array (1 for similar, 0 for dissimilar), threshold value\n",
    "        \"\"\"\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        if threshold == 'median':\n",
    "            thresh_val = np.median(scores)\n",
    "        elif threshold == 'mean':\n",
    "            thresh_val = np.mean(scores)\n",
    "        else:\n",
    "            thresh_val = threshold\n",
    "            \n",
    "        return (scores >= thresh_val).astype(int), thresh_val\n",
    "    \n",
    "    def compute_classification_metrics(self, human_scores, model_scores, \n",
    "                                       threshold='median', verbose=True):\n",
    "        \"\"\"\n",
    "        Compute precision, recall, accuracy, and F1 score for isiZulu embeddings.\n",
    "        \n",
    "        Treats word similarity as a binary classification problem:\n",
    "        - Similar pairs (high scores) vs. Dissimilar pairs (low scores)\n",
    "        \n",
    "        Args:\n",
    "            human_scores: Ground truth similarity scores\n",
    "            model_scores: Model-predicted similarity scores\n",
    "            threshold: How to binarize scores ('median', 'mean', or numeric value)\n",
    "            verbose: Whether to print detailed results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with classification metrics\n",
    "        \"\"\"\n",
    "        # Binarize scores\n",
    "        y_true, human_thresh = self.binarize_scores(human_scores, threshold)\n",
    "        y_pred, model_thresh = self.binarize_scores(model_scores, threshold)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix_np(y_true, y_pred)\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_np(tp, tn, fp, fn)\n",
    "        precision = precision_np(tp, fp)\n",
    "        recall = recall_np(tp, fn)\n",
    "        f1 = f1_np(precision, recall)\n",
    "        \n",
    "        # Create confusion matrix for compatibility\n",
    "        cm = np.array([[tn, fp], [fn, tp]])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìä Amamethrikhi Okuhlukanisa (Classification Metrics)\")\n",
    "            print(f\"  Threshold: {threshold}\")\n",
    "            print(f\"  Human threshold:  {human_thresh:.4f}\")\n",
    "            print(f\"  Model threshold:  {model_thresh:.4f}\")\n",
    "            print(f\"\\n  Ukunemba (Accuracy):   {accuracy:.4f}\")\n",
    "            print(f\"  Ukunembile (Precision): {precision:.4f}\")\n",
    "            print(f\"  Ukukhumbula (Recall):   {recall:.4f}\")\n",
    "            print(f\"  I-F1 Score:             {f1:.4f}\")\n",
    "            print(f\"\\n  Confusion Matrix:\")\n",
    "            print(f\"                    Okubikezelwe (Predicted)\")\n",
    "            print(f\"                  Akufani  Kuyafana\")\n",
    "            print(f\"    Iqiniso   Akufani  {tn:4d}   {fp:4d}\")\n",
    "            print(f\"    (Actual)  Kuyafana {fn:4d}   {tp:4d}\")\n",
    "            print(f\"\\n  Amaqiniso Apositivi (True Positives):  {tp}\")\n",
    "            print(f\"  Amaqiniso Anegativi (True Negatives):  {tn}\")\n",
    "            print(f\"  Amanga Apositivi (False Positives):     {fp}\")\n",
    "            print(f\"  Amanga Anegativi (False Negatives):     {fn}\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'true_positives': int(tp),\n",
    "            'true_negatives': int(tn),\n",
    "            'false_positives': int(fp),\n",
    "            'false_negatives': int(fn),\n",
    "            'human_threshold': human_thresh,\n",
    "            'model_threshold': model_thresh,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "    \n",
    "    def evaluate_similarity(self, word_pairs, compute_classification=True, \n",
    "                          threshold='median'):\n",
    "        \"\"\"\n",
    "        Evaluate mBERT model on word pairs with both correlation and classification metrics.\n",
    "        \n",
    "        Args:\n",
    "            word_pairs: List of tuples (word1, word2, human_score)\n",
    "            compute_classification: Whether to compute precision/recall/F1\n",
    "            threshold: Threshold for binarization ('median', 'mean', or numeric)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all evaluation results\n",
    "        \"\"\"\n",
    "        model_scores = []\n",
    "        human_scores = []\n",
    "        missing_pairs = []\n",
    "        valid_pairs = []\n",
    "        \n",
    "        print(f\"\\nüîÑ Computing similarities for {len(word_pairs)} word pairs...\")\n",
    "        \n",
    "        for i, (word1, word2, human_score) in enumerate(word_pairs):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Progress: {i+1}/{len(word_pairs)} pairs processed...\", end='\\r')\n",
    "            \n",
    "            sim = self.get_similarity(word1, word2)\n",
    "            \n",
    "            if sim is not None:\n",
    "                model_scores.append(sim)\n",
    "                human_scores.append(human_score)\n",
    "                valid_pairs.append((word1, word2))\n",
    "            else:\n",
    "                missing_pairs.append((word1, word2))\n",
    "        \n",
    "        print(f\"  Progress: {len(word_pairs)}/{len(word_pairs)} pairs processed... ‚úÖ\")\n",
    "        \n",
    "        # Compute correlations\n",
    "        if len(model_scores) < 2:\n",
    "            print(\"‚ùå Amapheyari awanele ukubala amamethrikhi (Not enough valid pairs)\")\n",
    "            return None\n",
    "        \n",
    "        spearman_corr, spearman_pval = spearmanr(human_scores, model_scores)\n",
    "        pearson_corr, pearson_pval = pearsonr(human_scores, model_scores)\n",
    "        \n",
    "        coverage = len(model_scores) / len(word_pairs) * 100\n",
    "        \n",
    "        # Print correlation results\n",
    "        print(f\"\\nüìä Amamethrikhi Wokuhlobana (Correlation Metrics):\")\n",
    "        print(f\"  Ukumbozwa:    {len(model_scores)}/{len(word_pairs)} pairs ({coverage:.1f}%)\")\n",
    "        print(f\"  Spearman œÅ:   {spearman_corr:.4f} (p={spearman_pval:.4e})\")\n",
    "        print(f\"  Pearson r:    {pearson_corr:.4f} (p={pearson_pval:.4e})\")\n",
    "        \n",
    "        results = {\n",
    "            'spearman': spearman_corr,\n",
    "            'spearman_pval': spearman_pval,\n",
    "            'pearson': pearson_corr,\n",
    "            'pearson_pval': pearson_pval,\n",
    "            'coverage': coverage,\n",
    "            'n_pairs': len(model_scores),\n",
    "            'total_pairs': len(word_pairs),\n",
    "            'missing_pairs': missing_pairs,\n",
    "            'valid_pairs': valid_pairs\n",
    "        }\n",
    "        \n",
    "        # Compute classification metrics\n",
    "        if compute_classification:\n",
    "            classification_results = self.compute_classification_metrics(\n",
    "                human_scores, model_scores, threshold=threshold, verbose=True\n",
    "            )\n",
    "            results.update(classification_results)\n",
    "        \n",
    "        if missing_pairs:\n",
    "            print(f\"\\n‚ö†Ô∏è  Missing/error pairs: {len(missing_pairs)}\")\n",
    "            if len(missing_pairs) <= 10:\n",
    "                for w1, w2 in missing_pairs:\n",
    "                    print(f\"  - {w1}, {w2}\")\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def load_isizulu_similarity_dataset(self, filepath):\n",
    "        \"\"\"\n",
    "        Load isiZulu word similarity dataset.\n",
    "        \n",
    "        Expected format (CSV or TSV):\n",
    "        word1, word2, similarity_score\n",
    "        \n",
    "        Example:\n",
    "        umfazi,indoda,5.2\n",
    "        ingane,umntwana,9.5\n",
    "        inja,ikati,6.8\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (word1, word2, similarity_score)\n",
    "        \"\"\"\n",
    "        word_pairs = []\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ö†Ô∏è  Ifayela ayitholakali (File not found): {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        # Detect delimiter\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            delimiter = '\\t' if '\\t' in first_line else ','\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=delimiter)\n",
    "            \n",
    "            # Check if there's a header\n",
    "            first_row = next(reader)\n",
    "            if not first_row[0].replace('.','').replace('-','').isdigit():\n",
    "                # Has header, skip it\n",
    "                pass\n",
    "            else:\n",
    "                # No header, process first row\n",
    "                if len(first_row) >= 3:\n",
    "                    word1 = first_row[0].strip().lower()\n",
    "                    word2 = first_row[1].strip().lower()\n",
    "                    score = float(first_row[2])\n",
    "                    word_pairs.append((word1, word2, score))\n",
    "            \n",
    "            for row in reader:\n",
    "                if len(row) >= 3:\n",
    "                    word1 = row[0].strip().lower()\n",
    "                    word2 = row[1].strip().lower()\n",
    "                    try:\n",
    "                        score = float(row[2])\n",
    "                        word_pairs.append((word1, word2, score))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"‚úÖ Kulayishwe amapheyari {len(word_pairs)} amagama (Loaded {len(word_pairs)} word pairs)\")\n",
    "        return word_pairs\n",
    "    \n",
    "    def evaluate_isizulu_dataset(self, filepath, \n",
    "                                  compute_classification=True, \n",
    "                                  threshold='median'):\n",
    "        \"\"\"\n",
    "        Evaluate mBERT model on isiZulu word similarity dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä Ukuhlolwa Kwedathasethe YesiZulu (isiZulu Dataset Evaluation)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        word_pairs = self.load_isizulu_similarity_dataset(filepath)\n",
    "        if word_pairs is None:\n",
    "            return None\n",
    "        \n",
    "        return self.evaluate_similarity(word_pairs, compute_classification, threshold)\n",
    "    \n",
    "    def compare_pooling_strategies(self, word_pairs, pooling_methods=['mean', 'first', 'last', 'max']):\n",
    "        \"\"\"\n",
    "        Compare different pooling strategies for subword tokens.\n",
    "        \n",
    "        Args:\n",
    "            word_pairs: List of tuples (word1, word2, human_score)\n",
    "            pooling_methods: List of pooling strategies to compare\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results for each pooling method\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üî¨ POOLING STRATEGY COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for pooling in pooling_methods:\n",
    "            print(f\"\\nüìä Testing pooling method: {pooling.upper()}\")\n",
    "            print(\"‚îÄ\"*70)\n",
    "            \n",
    "            # Temporarily change pooling method\n",
    "            original_pooling = self.pooling\n",
    "            self.pooling = pooling\n",
    "            self.similarity_cache = {}  # Clear cache\n",
    "            \n",
    "            # Evaluate\n",
    "            result = self.evaluate_similarity(word_pairs, compute_classification=True, threshold='median')\n",
    "            \n",
    "            if result:\n",
    "                results[pooling] = result\n",
    "            \n",
    "            # Restore original pooling\n",
    "            self.pooling = original_pooling\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìà POOLING STRATEGY SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for pooling, data in results.items():\n",
    "            print(f\"\\n{pooling.upper()}:\")\n",
    "            print(f\"  Spearman œÅ: {data['spearman']:.4f}\")\n",
    "            print(f\"  Pearson r:  {data['pearson']:.4f}\")\n",
    "            print(f\"  Accuracy:   {data['accuracy']:.4f}\")\n",
    "            print(f\"  F1 Score:   {data['f1_score']:.4f}\")\n",
    "        \n",
    "        # Find best\n",
    "        best_spearman = max(results.items(), key=lambda x: x[1]['spearman'])\n",
    "        best_f1 = max(results.items(), key=lambda x: x[1]['f1_score'])\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Spearman œÅ: {best_spearman[0].upper()} ({best_spearman[1]['spearman']:.4f})\")\n",
    "        print(f\"üèÜ Best F1 Score:   {best_f1[0].upper()} ({best_f1[1]['f1_score']:.4f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_layers(self, word_pairs, layers=[-4, -3, -2, -1]):\n",
    "        \"\"\"\n",
    "        Compare different BERT layers for word embeddings.\n",
    "        \n",
    "        Args:\n",
    "            word_pairs: List of tuples (word1, word2, human_score)\n",
    "            layers: List of layer indices to compare (-1 = last layer)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results for each layer\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üî¨ BERT LAYER COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for layer_idx in layers:\n",
    "            print(f\"\\nüìä Testing layer: {layer_idx}\")\n",
    "            print(\"‚îÄ\"*70)\n",
    "            \n",
    "            # Temporarily change layer\n",
    "            original_layer = self.layer\n",
    "            self.layer = layer_idx\n",
    "            self.similarity_cache = {}  # Clear cache\n",
    "            \n",
    "            # Evaluate\n",
    "            result = self.evaluate_similarity(word_pairs, compute_classification=True, threshold='median')\n",
    "            \n",
    "            if result:\n",
    "                results[f\"layer_{layer_idx}\"] = result\n",
    "            \n",
    "            # Restore original layer\n",
    "            self.layer = original_layer\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìà LAYER COMPARISON SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for layer_name, data in results.items():\n",
    "            print(f\"\\n{layer_name.upper()}:\")\n",
    "            print(f\"  Spearman œÅ: {data['spearman']:.4f}\")\n",
    "            print(f\"  Pearson r:  {data['pearson']:.4f}\")\n",
    "            print(f\"  Accuracy:   {data['accuracy']:.4f}\")\n",
    "            print(f\"  F1 Score:   {data['f1_score']:.4f}\")\n",
    "        \n",
    "        # Find best\n",
    "        best_spearman = max(results.items(), key=lambda x: x[1]['spearman'])\n",
    "        best_f1 = max(results.items(), key=lambda x: x[1]['f1_score'])\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Spearman œÅ: {best_spearman[0].upper()} ({best_spearman[1]['spearman']:.4f})\")\n",
    "        print(f\"üèÜ Best F1 Score:   {best_f1[0].upper()} ({best_f1[1]['f1_score']:.4f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results):\n",
    "        \"\"\"Print a comprehensive summary of all results in English and isiZulu.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìà ISIFINYEZO ESIPHELELE (COMPREHENSIVE SUMMARY)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for benchmark, data in results.items():\n",
    "            print(f\"\\n{benchmark.upper()}:\")\n",
    "            print(f\"  {'‚îÄ'*60}\")\n",
    "            print(f\"  Amamethrikhi Wokuhlobana (Correlation Metrics):\")\n",
    "            print(f\"    Spearman œÅ:   {data['spearman']:.4f}\")\n",
    "            print(f\"    Pearson r:    {data['pearson']:.4f}\")\n",
    "            print(f\"    Ukumbozwa:    {data['coverage']:.1f}%\")\n",
    "            \n",
    "            if 'accuracy' in data:\n",
    "                print(f\"\\n  Amamethrikhi Okuhlukanisa (Classification Metrics):\")\n",
    "                print(f\"    Ukunemba (Accuracy):    {data['accuracy']:.4f}\")\n",
    "                print(f\"    Ukunembile (Precision): {data['precision']:.4f}\")\n",
    "                print(f\"    Ukukhumbula (Recall):   {data['recall']:.4f}\")\n",
    "                print(f\"    I-F1 Score:             {data['f1_score']:.4f}\")\n",
    "\n",
    "\n",
    "def create_sample_isizulu_dataset():\n",
    "    \"\"\"Create a sample isiZulu word similarity dataset for testing.\"\"\"\n",
    "    sample_data = \"\"\"igama1,igama2,isilinganiso\n",
    "umfazi,indoda,5.2\n",
    "ingane,umntwana,9.5\n",
    "inja,ikati,6.8\n",
    "indlu,ikhaya,8.7\n",
    "umfula,ulwandle,6.5\n",
    "isikole,isikhungo,7.8\n",
    "imali,uhulumeni,4.2\n",
    "uthisha,umfundi,7.5\n",
    "isitsha,indishi,9.2\n",
    "ibhola,umdlalo,7.8\n",
    "ukudla,ukuphuza,6.5\n",
    "usuku,ubusuku,3.2\n",
    "umuntu,ubuntu,8.5\n",
    "itheku,idolobha,9.1\n",
    "izulu,umoya,6.8\"\"\"\n",
    "    \n",
    "    with open('isizulu_similarity_sample.csv', 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_data)\n",
    "    \n",
    "    print(\"‚úÖ Kwakhiwe i-isizulu_similarity_sample.csv (Created isiZulu sample dataset)\")\n",
    "    return 'isizulu_similarity_sample.csv'\n",
    "\n",
    "\n",
    "# Demo usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä IsiZulu Word Similarity Benchmark Evaluator - mBERT VERSION\")\n",
    "    print(\"   Okuqukethwe: Precision, Recall, Accuracy, ne-F1 Score\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚úÖ mBERT-compatible implementation!\")\n",
    "    print(\"‚úÖ Supports contextualized embeddings with multiple pooling strategies\")\n",
    "    print(\"‚úÖ Can compare different BERT layers\")\n",
    "    print(\"‚úÖ Ready to use with multilingual BERT!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìñ QUICK START\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "from isizulu_benchmark_evaluator_mbert import IsiZuluBenchmarkEvaluatorMBERT\n",
    "\n",
    "# Create evaluator (will automatically load mBERT)\n",
    "evaluator = IsiZuluBenchmarkEvaluatorMBERT(\n",
    "    pooling='mean',  # or 'first', 'last', 'max'\n",
    "    layer=-1         # -1 = last layer, -2 = second-to-last, etc.\n",
    ")\n",
    "\n",
    "# Evaluate on your isiZulu dataset\n",
    "results = evaluator.evaluate_isizulu_dataset(\n",
    "    'isizulu_word_similarity.csv',\n",
    "    compute_classification=True,\n",
    "    threshold='median'\n",
    ")\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Spearman œÅ: {results['spearman']:.4f}\")\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall: {results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "\n",
    "# Compare pooling strategies\n",
    "word_pairs = evaluator.load_isizulu_similarity_dataset('isizulu_word_similarity.csv')\n",
    "pooling_results = evaluator.compare_pooling_strategies(word_pairs)\n",
    "\n",
    "# Compare BERT layers\n",
    "layer_results = evaluator.compare_layers(word_pairs, layers=[-4, -3, -2, -1])\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üß™ Running DEMO with sample data...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create sample dataset\n",
    "    sample_file = create_sample_isizulu_dataset()\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    print(\"\\nüì• Initializing mBERT evaluator...\")\n",
    "    evaluator = IsiZuluBenchmarkEvaluatorMBERT(pooling='mean', layer=-1)\n",
    "    \n",
    "    # Evaluate sample dataset\n",
    "    print(\"\\nüìä Evaluating sample isiZulu dataset...\")\n",
    "    results = evaluator.evaluate_isizulu_dataset(\n",
    "        sample_file, \n",
    "        compute_classification=True, \n",
    "        threshold='median'\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    if results:\n",
    "        evaluator.print_summary({'Sample Dataset': results})\n",
    "    \n",
    "    # Compare pooling strategies (optional, comment out if too slow)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üî¨ Comparing pooling strategies (this may take a minute)...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    word_pairs = evaluator.load_isizulu_similarity_dataset(sample_file)\n",
    "    if word_pairs:\n",
    "        pooling_results = evaluator.compare_pooling_strategies(\n",
    "            word_pairs[:5],  # Use only first 5 pairs for demo\n",
    "            pooling_methods=['mean', 'first']\n",
    "        )\n",
    "    \n",
    "    print(\"\\nüöÄ Demo evaluation completed!\")\n",
    "    print(\"\\nüí° TIP: For full evaluation, use your real isiZulu similarity dataset\")\n",
    "    print(\"   and consider running overnight for large datasets (mBERT is slower than Word2Vec)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
