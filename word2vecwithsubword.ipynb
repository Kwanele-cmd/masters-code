{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FastText' from 'gensim.models.word2vec' (C:\\Users\\USER-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\word2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr, pearsonr\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mword2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastText\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# --- COSINE SIMILARITY ---\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcosine_similarity\u001b[39m(vec1, vec2):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FastText' from 'gensim.models.word2vec' (C:\\Users\\USER-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\word2vec.py)"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# --- AUTO-INSTALLER BLOCK ---\n",
    "def maintain_dependencies():\n",
    "    required_libraries = ['numpy', 'scipy', 'gensim']\n",
    "    for lib in required_libraries:\n",
    "        try:\n",
    "            __import__(lib)\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Library '{lib}' not found. Installing now...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
    "\n",
    "maintain_dependencies()\n",
    "# ----------------------------\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# --- COSINE SIMILARITY ---\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity: (A ¬∑ B) / (||A|| √ó ||B||)\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# --- CLASSIFICATION METRICS ---\n",
    "def confusion_matrix_np(y_true, y_pred):\n",
    "    \"\"\"Calculate confusion matrix components\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def accuracy_np(tp, tn, fp, fn):\n",
    "    \"\"\"Accuracy = (TP + TN) / (TP + TN + FP + FN)\"\"\"\n",
    "    total = tp + tn + fp + fn\n",
    "    return (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "def precision_np(tp, fp):\n",
    "    \"\"\"Precision = TP / (TP + FP)\"\"\"\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "\n",
    "def recall_np(tp, fn):\n",
    "    \"\"\"Recall = TP / (TP + FN)\"\"\"\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "def f1_np(precision, recall):\n",
    "    \"\"\"F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\"\"\"\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "# --- FILE LOADING ---\n",
    "def load_text_file(filepath):\n",
    "    \"\"\"Reads a .txt file and returns a list of tokenized sentences.\"\"\"\n",
    "    sentences = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.lower().strip().split()\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "        return sentences\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- CREATE SAMPLE CORPUS ---\n",
    "def create_sample_corpus(filepath):\n",
    "    \"\"\"Creates a sample isiZulu corpus for testing\"\"\"\n",
    "    sample_corpus = \"\"\"umfazi nendoda bahamba esikoleni\n",
    "ingane idla ukudla kwayo\n",
    "inja ikati zidlala eyadini\n",
    "isikole isikhungo semfundo\n",
    "ikhaya indlu yomndeni\n",
    "umfula ulwandle amanzi\n",
    "uthisha umfundi bafunda\n",
    "isitsha indishi kudla\n",
    "ibhola umdlalo imidlalo\n",
    "umuntu ubuntu ubuntu\n",
    "itheku idolobha amadolobha\n",
    "incwadi iphepha ukubhala\n",
    "ikhompiyutha ikhibhodi theknoloji\n",
    "indiza imoto isitimela ukuhamba\n",
    "ucingo ukuxhumana uxhumano\n",
    "umabonakude umsakazo ezindaba\n",
    "abezindaba umsakazo ukubika\n",
    "udokotela umhlengikazi ukwelapha\n",
    "solwazi umfundi ukufunda\n",
    "inkampani amasheya ukuhweba\n",
    "isitoko indali ukuthenga\n",
    "ibhange imali ukonga\n",
    "ukhuni ihlathi amahlathi\n",
    "inkosi indlovukazi umbuso\n",
    "umbhishobhi uRabi unkulunkulu\n",
    "inyoni iqhude izilwane\n",
    "ithuluzi ukusebenza\n",
    "umfana mfowethu umndeni\n",
    "uhambo imoto ukuhamba\n",
    "imali idola ingcebo impahla\n",
    "imali ibhange ukufaka ukuhoxa ukuwasha\n",
    "ihlosi isilwane izilwane i-zoo\n",
    "usho njalo isikhathi\n",
    "uqhawekazi mdikane isibindi\n",
    "umphiko indiza\n",
    "usuku ubusuku isikhathi\n",
    "inzondo ucansi\n",
    "isinkwa ibhotela ukudla\n",
    "ikhukhamba izambane imifino\n",
    "hlakaniphile isilima ukuhlakanipha\n",
    "ukuzala iqanda\n",
    "umtapo wezincwadi incwadi\n",
    "igwaba inkosi\n",
    "qalisa ithuluzi ukusebenza\n",
    "ukuhlukumeza isidakamizwa\n",
    "imeya inkosi amadolobha\n",
    "isikweletu imali ukuboleka\n",
    "umasipala uhulumeni isikhungo\n",
    "inkohlakalo icala ububi\n",
    "inyuvesi isikole ukufunda\n",
    "isivivinyo u-matric ukuhlola\n",
    "ingoma umculo icwecwe\n",
    "umrepha umculi umculo\n",
    "ikhwaya umbhalo amaculo\n",
    "idume izindondo udumo\n",
    "amaphoyisa abasolwa icala\n",
    "isibhamu inhlamvu udubula\n",
    "ubunhloli umkhondo uphenyisiso\n",
    "isiteshi inkantolo amaphoyisa\n",
    "ilokishi idolobha indawo\n",
    "ihhotela isivakashi ukulala\n",
    "ingqalasizinda ukuthuthukiswa ukwakha\n",
    "emakhaya iphesheya ezweni\n",
    "itekisi imoto ukuhamba\n",
    "ubudokotela udokotela ukwelapha\n",
    "isifo impilo ukugula\n",
    "ubumnandi ukujabula injabulo\n",
    "\"\"\"\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_corpus)\n",
    "    print(f\"‚úÖ Sample corpus created: {filepath}\")\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    CORPUS_FILE = 'isizulu_corpus.txt'\n",
    "    OUTPUT_CSV = 'isizulu_fasttext_ngram_results.csv'\n",
    "    \n",
    "    # Test pairs\n",
    "    isi_test_pairs = [\n",
    "        ('inkosi', 'imeya', 8.45),\n",
    "        ('imali', 'isikweletu', 7.12),\n",
    "        ('uhulumeni', 'umasipala', 8.90),\n",
    "        ('inkohlakalo', 'icala', 7.50),\n",
    "        \n",
    "        # Imfundo no-Matric\n",
    "        ('isikole', 'inyuvesi', 8.20),\n",
    "        ('umfundi', 'uthisha', 7.65),\n",
    "        ('izifundo', 'imiphumela', 6.80),\n",
    "        ('u-matric', 'isivivinyo', 9.10),\n",
    "        \n",
    "        # Umculo Nobuciko\n",
    "        ('ingoma', 'icwecwe', 8.55),\n",
    "        ('umrepha', 'umculi', 9.25),\n",
    "        ('ikhwaya', 'umbhalo', 4.10),\n",
    "        ('idume', 'izindondo', 6.40),\n",
    "        \n",
    "        # Amaphoyisa Nobugebengu\n",
    "        ('amaphoyisa', 'abasolwa', 8.15),\n",
    "        ('isibhamu', 'inhlamvu', 9.40),\n",
    "        ('ubunhloli', 'umkhondo', 8.70),\n",
    "        ('isiteshi', 'inkantolo', 6.95),\n",
    "        \n",
    "        # Indawo Nezokuvakasha\n",
    "        ('idolobha', 'ilokishi', 7.30),\n",
    "        ('isivakashi', 'ihhotela', 8.85),\n",
    "        ('ingqalasizinda', 'ukuthuthukiswa', 7.75),\n",
    "        ('emakhaya', 'iphesheya', 3.20),\n",
    "\n",
    "        # Amagama Angahlobene (Negative Controls)\n",
    "        ('itekisi', 'ubudokotela', 1.15),\n",
    "        ('umculo', 'isifo', 0.90),\n",
    "        ('u-matric', 'ubumnandi', 2.50),\n",
    "        ('inkosi', 'igwaba', 1.05),\n",
    "    ]\n",
    "\n",
    "    # Create sample corpus if it doesn't exist\n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        print(f\"üìù Creating sample corpus file...\")\n",
    "        create_sample_corpus(CORPUS_FILE)\n",
    "\n",
    "    print(f\"\\nüìÇ Loading corpus from {CORPUS_FILE}...\")\n",
    "    sentences = load_text_file(CORPUS_FILE)\n",
    "    \n",
    "    if not sentences:\n",
    "        print(\"üõë The text file is empty.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(sentences)} sentences\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Training FastText model with subword n-grams...\")\n",
    "    print(f\"   Architecture: Skip-gram with character n-grams\")\n",
    "    print(f\"   Parameters:\")\n",
    "    print(f\"      - Vector size: 200\")\n",
    "    print(f\"      - Window: 7\")\n",
    "    print(f\"      - Min n-gram: 3 (trigrams)\")\n",
    "    print(f\"      - Max n-gram: 6 (hexagrams)\")\n",
    "    print(f\"      - Epochs: 100\")\n",
    "    print(f\"      - Min count: 2\")\n",
    "    print(f\"   This enables handling of OOV words and morphological variations!\\n\")\n",
    "    \n",
    "    # Flush output to ensure it's displayed\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Train FastText model with subword information\n",
    "    model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=200,\n",
    "        window=7,\n",
    "        min_count=2,\n",
    "        epochs=100,\n",
    "        sg=1,  # Skip-gram architecture\n",
    "        workers=4,\n",
    "        alpha=0.025,\n",
    "        min_alpha=0.0001,\n",
    "        negative=10,\n",
    "        sample=1e-1,\n",
    "        # Subword n-gram parameters\n",
    "        min_n=3,  # Minimum character n-gram length\n",
    "        max_n=6,  # Maximum character n-gram length\n",
    "        word_ngrams=1,  # Use word n-grams\n",
    "        bucket=2000000  # Hash bucket size for n-grams\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ FastText model trained! Vocabulary size: {len(model.wv)}\")\n",
    "    print(f\"   Subword n-grams: {model.wv.min_n}-{model.wv.max_n} characters\")\n",
    "    print(f\"   This model can now handle unseen words!\\n\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    print(\"=\"*90)\n",
    "    print(\"CALCULATING COSINE SIMILARITIES (WITH SUBWORD N-GRAMS)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    cosine_scores = []\n",
    "    human_scores = []\n",
    "    results = []\n",
    "    \n",
    "    print(f\"{'Word 1':<25} {'Word 2':<25} {'Human':<10} {'Cosine':<10} {'Status':<15}\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    for w1, w2, h_score in isi_test_pairs:\n",
    "        try:\n",
    "            # FastText can handle OOV words using subword information\n",
    "            vec1 = model.wv[w1]\n",
    "            vec2 = model.wv[w2]\n",
    "            cos_sim = cosine_similarity(vec1, vec2)\n",
    "            \n",
    "            # Check if words are in vocabulary or computed from n-grams\n",
    "            in_vocab_w1 = w1 in model.wv.key_to_index\n",
    "            in_vocab_w2 = w2 in model.wv.key_to_index\n",
    "            \n",
    "            if in_vocab_w1 and in_vocab_w2:\n",
    "                status = \"In Vocab\"\n",
    "            elif in_vocab_w1 or in_vocab_w2:\n",
    "                status = \"Partial N-gram\"\n",
    "            else:\n",
    "                status = \"Full N-gram\"\n",
    "            \n",
    "            print(f\"{w1:<25} {w2:<25} {h_score:<10.2f} {cos_sim:<10.6f} {status:<15}\")\n",
    "            \n",
    "            cosine_scores.append(cos_sim)\n",
    "            human_scores.append(h_score)\n",
    "            \n",
    "            results.append({\n",
    "                'word1': w1,\n",
    "                'word2': w2,\n",
    "                'human_score': h_score,\n",
    "                'cosine_similarity': cos_sim,\n",
    "                'in_vocab': status\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"{w1:<25} {w2:<25} {h_score:<10.2f} {'ERROR':<10} {str(e):<15}\")\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if len(cosine_scores) < 2:\n",
    "        print(\"‚ùå Not enough valid pairs to calculate metrics.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # --- CALCULATE ALL METRICS ---\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"EVALUATION METRICS (FASTTEXT WITH SUBWORD N-GRAMS)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # 1. Correlation Metrics\n",
    "    rho, rho_p = spearmanr(human_scores, cosine_scores)\n",
    "    pear, pear_p = pearsonr(human_scores, cosine_scores)\n",
    "    \n",
    "    print(f\"\\nüìä CORRELATION METRICS:\")\n",
    "    print(f\"   Spearman Correlation:     {rho:.6f} (p-value: {rho_p:.6f})\")\n",
    "    print(f\"   Pearson Correlation:      {pear:.6f} (p-value: {pear_p:.6f})\")\n",
    "    \n",
    "    # 2. Classification Metrics (binarize using median)\n",
    "    human_median = np.median(human_scores)\n",
    "    cosine_median = np.median(cosine_scores)\n",
    "    \n",
    "    y_true = (np.array(human_scores) >= human_median).astype(int)\n",
    "    y_pred = (np.array(cosine_scores) >= cosine_median).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix_np(y_true, y_pred)\n",
    "    accuracy = accuracy_np(tp, tn, fp, fn)\n",
    "    precision = precision_np(tp, fp)\n",
    "    recall = recall_np(tp, fn)\n",
    "    f1 = f1_np(precision, recall)\n",
    "    \n",
    "    print(f\"\\nüìà CLASSIFICATION METRICS (Median Threshold):\")\n",
    "    print(f\"   Accuracy:                 {accuracy:.6f}\")\n",
    "    print(f\"   Precision:                {precision:.6f}\")\n",
    "    print(f\"   Recall:                   {recall:.6f}\")\n",
    "    print(f\"   F1 Score:                 {f1:.6f}\")\n",
    "    \n",
    "    print(f\"\\nüîç CONFUSION MATRIX:\")\n",
    "    print(f\"   True Positives (TP):      {tp}\")\n",
    "    print(f\"   True Negatives (TN):      {tn}\")\n",
    "    print(f\"   False Positives (FP):     {fp}\")\n",
    "    print(f\"   False Negatives (FN):     {fn}\")\n",
    "    \n",
    "    print(f\"\\nüìù COVERAGE:\")\n",
    "    print(f\"   Pairs Evaluated:          {len(cosine_scores)}/{len(isi_test_pairs)}\")\n",
    "    print(f\"   Coverage Rate:            {len(cosine_scores)/len(isi_test_pairs)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä SCORE STATISTICS:\")\n",
    "    print(f\"   Cosine Similarity:\")\n",
    "    print(f\"      Min:     {min(cosine_scores):.6f}\")\n",
    "    print(f\"      Max:     {max(cosine_scores):.6f}\")\n",
    "    print(f\"      Mean:    {np.mean(cosine_scores):.6f}\")\n",
    "    print(f\"      Median:  {cosine_median:.6f}\")\n",
    "    print(f\"      Std Dev: {np.std(cosine_scores):.6f}\")\n",
    "    print(f\"   Human Scores:\")\n",
    "    print(f\"      Min:     {min(human_scores):.2f}\")\n",
    "    print(f\"      Max:     {max(human_scores):.2f}\")\n",
    "    print(f\"      Mean:    {np.mean(human_scores):.2f}\")\n",
    "    print(f\"      Median:  {human_median:.2f}\")\n",
    "    print(f\"      Std Dev: {np.std(human_scores):.2f}\")\n",
    "    \n",
    "    print(f\"\\nüî§ SUBWORD N-GRAM BENEFITS:\")\n",
    "    print(f\"   - Can handle out-of-vocabulary (OOV) words\")\n",
    "    print(f\"   - Captures morphological similarities in isiZulu\")\n",
    "    print(f\"   - Character n-grams: {model.wv.min_n} to {model.wv.max_n}\")\n",
    "    print(f\"   - Example: 'inkosi' ‚Üí n-grams like 'ink', 'nko', 'kos', 'osi', 'inko', 'nkos', 'kosi'\")\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['word1', 'word2', 'human_score', 'cosine_similarity', 'in_vocab']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results saved to '{OUTPUT_CSV}'\")\n",
    "    \n",
    "    # Save metrics summary\n",
    "    metrics_file = 'fasttext_ngram_evaluation_metrics.txt'\n",
    "    with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"ISIZULU WORD SIMILARITY EVALUATION (FASTTEXT WITH SUBWORD N-GRAMS)\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(\"MODEL CONFIGURATION:\\n\")\n",
    "        f.write(f\"  Architecture:          FastText (Skip-gram with subword n-grams)\\n\")\n",
    "        f.write(f\"  Character n-grams:     {model.wv.min_n}-{model.wv.max_n}\\n\")\n",
    "        f.write(f\"  Vector size:           200\\n\")\n",
    "        f.write(f\"  Window:                7\\n\")\n",
    "        f.write(f\"  Epochs:                100\\n\\n\")\n",
    "        f.write(\"CORRELATION METRICS:\\n\")\n",
    "        f.write(f\"  Spearman Correlation:  {rho:.6f} (p={rho_p:.6f})\\n\")\n",
    "        f.write(f\"  Pearson Correlation:   {pear:.6f} (p={pear_p:.6f})\\n\\n\")\n",
    "        f.write(\"CLASSIFICATION METRICS:\\n\")\n",
    "        f.write(f\"  Accuracy:              {accuracy:.6f}\\n\")\n",
    "        f.write(f\"  Precision:             {precision:.6f}\\n\")\n",
    "        f.write(f\"  Recall:                {recall:.6f}\\n\")\n",
    "        f.write(f\"  F1 Score:              {f1:.6f}\\n\\n\")\n",
    "        f.write(\"CONFUSION MATRIX:\\n\")\n",
    "        f.write(f\"  TP: {tp}  FP: {fp}\\n\")\n",
    "        f.write(f\"  FN: {fn}  TN: {tn}\\n\\n\")\n",
    "        f.write(\"COVERAGE:\\n\")\n",
    "        f.write(f\"  Pairs Evaluated: {len(cosine_scores)}/{len(isi_test_pairs)}\\n\")\n",
    "        f.write(f\"  Coverage Rate:   {len(cosine_scores)/len(isi_test_pairs)*100:.2f}%\\n\\n\")\n",
    "        f.write(\"SUBWORD N-GRAM ADVANTAGES:\\n\")\n",
    "        f.write(\"  - Handles out-of-vocabulary words\\n\")\n",
    "        f.write(\"  - Captures morphological patterns in agglutinative languages\\n\")\n",
    "        f.write(\"  - Better generalization for rare words\\n\")\n",
    "        f.write(\"  - Robust to typos and spelling variations\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Metrics summary saved to '{metrics_file}'\")\n",
    "    \n",
    "    # Demonstrate OOV capability\n",
    "    print(f\"\\nüî¨ DEMONSTRATING OOV WORD HANDLING:\")\n",
    "    print(\"=\"*90)\n",
    "    test_oov_words = ['ukuthenga', 'abantwana', 'izinkomo']\n",
    "    for test_word in test_oov_words:\n",
    "        try:\n",
    "            vec = model.wv[test_word]\n",
    "            in_vocab = test_word in model.wv.key_to_index\n",
    "            status = \"In vocabulary\" if in_vocab else \"Computed from n-grams\"\n",
    "            print(f\"   '{test_word}': {status} ‚úì\")\n",
    "        except:\n",
    "            print(f\"   '{test_word}': Failed ‚úó\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    print(\"\\nüéâ FastText evaluation with subword n-grams complete!\")\n",
    "    print(\"   Subword n-grams enable better handling of morphologically rich languages like isiZulu!\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
