{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1592693646.py, line 763)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 763\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor name, count, color\u001b[39m\n                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Quick check for packages without subprocess (much faster)\n",
    "def check_packages():\n",
    "    \"\"\"Quick check for required packages.\"\"\"\n",
    "    missing = []\n",
    "    optional_missing = []\n",
    "    \n",
    "    # Core packages (required)\n",
    "    for package in ['nltk', 'gensim', 'scipy', 'numpy']:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing.append(package)\n",
    "    \n",
    "    # Visualization package (optional)\n",
    "    try:\n",
    "        __import__('matplotlib')\n",
    "        HAS_MATPLOTLIB = True\n",
    "    except ImportError:\n",
    "        optional_missing.append('matplotlib')\n",
    "        HAS_MATPLOTLIB = False\n",
    "    \n",
    "    if missing:\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚ö†Ô∏è  MISSING REQUIRED PACKAGES\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nMissing: {', '.join(missing)}\")\n",
    "        print(f\"\\nüí° Install with: pip install {' '.join(missing)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if optional_missing:\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚ÑπÔ∏è  OPTIONAL PACKAGES MISSING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nMissing: matplotlib (for visualization)\")\n",
    "        print(f\"üí° Install with: pip install matplotlib\")\n",
    "        print(f\"\\n‚úÖ Running in TEXT-ONLY mode (no visualization)\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return HAS_MATPLOTLIB\n",
    "\n",
    "HAS_MATPLOTLIB = check_packages()\n",
    "\n",
    "# Fast imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "if HAS_MATPLOTLIB:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK data only if needed\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading NLTK punkt_tab...\")\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"üì• Downloading NLTK punkt...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"üì• Downloading stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "def manual_pca_2d(vectors):\n",
    "    \"\"\"\n",
    "    FAST Manual PCA using SVD (much faster than eigenvalue decomposition).\n",
    "    \n",
    "    Args:\n",
    "        vectors: numpy array of shape (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_samples, 2) with PCA-reduced coordinates\n",
    "    \"\"\"\n",
    "    mean = np.mean(vectors, axis=0)\n",
    "    centered = vectors - mean\n",
    "    \n",
    "    # Use SVD for fast PCA\n",
    "    U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "    pca_result = U[:, :2] * S[:2]\n",
    "    \n",
    "    return pca_result\n",
    "\n",
    "\n",
    "def calculate_metrics(true_positives, false_positives, false_negatives, true_negatives=None):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1-score, and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        true_positives: Number of true positive predictions\n",
    "        false_positives: Number of false positive predictions\n",
    "        false_negatives: Number of false negative predictions\n",
    "        true_negatives: Number of true negative predictions (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1_score, and accuracy\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    if true_positives + false_positives > 0:\n",
    "        metrics['precision'] = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        metrics['precision'] = 0.0\n",
    "    \n",
    "    # Recall: TP / (TP + FN)\n",
    "    if true_positives + false_negatives > 0:\n",
    "        metrics['recall'] = true_positives / (true_positives + false_negatives)\n",
    "    else:\n",
    "        metrics['recall'] = 0.0\n",
    "    \n",
    "    # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    if metrics['precision'] + metrics['recall'] > 0:\n",
    "        metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
    "    else:\n",
    "        metrics['f1_score'] = 0.0\n",
    "    \n",
    "    # Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "    if true_negatives is not None:\n",
    "        total = true_positives + true_negatives + false_positives + false_negatives\n",
    "        if total > 0:\n",
    "            metrics['accuracy'] = (true_positives + true_negatives) / total\n",
    "        else:\n",
    "            metrics['accuracy'] = 0.0\n",
    "    else:\n",
    "        metrics['accuracy'] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "class Word2VecEvaluator:\n",
    "    \"\"\"\n",
    "    Optimized Word2Vec model evaluation toolkit.\n",
    "    \n",
    "    Features:\n",
    "    - Vocabulary exploration and statistics\n",
    "    - Analogy evaluation\n",
    "    - Similarity testing with correlation metrics\n",
    "    - Categorization assessment\n",
    "    - 2D visualizations using fast PCA\n",
    "    - Interactive testing modes\n",
    "    - Vector caching for improved performance\n",
    "    \n",
    "    Example:\n",
    "        >>> evaluator = Word2VecEvaluator(model_path='model.bin')\n",
    "        >>> evaluator.show_vocabulary(limit=20)\n",
    "        >>> evaluator.visualize_word_neighbors('king', topn=15)\n",
    "        >>> analogies = [(\"king\", \"man\", \"queen\", \"woman\")]\n",
    "        >>> metrics, results = evaluator.evaluate_analogies(analogies)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, model=None, cache_size=1000):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to saved Word2Vec model\n",
    "            model: Pre-loaded Word2Vec model object\n",
    "            cache_size: Maximum number of word vectors to cache (default: 1000)\n",
    "        \"\"\"\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.model = Word2Vec.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either model_path or model object\")\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self._word_cache = {}\n",
    "        self._cache_size = cache_size\n",
    "    \n",
    "    def _get_vector(self, word):\n",
    "        \"\"\"Get word vector with caching for performance.\"\"\"\n",
    "        if word not in self._word_cache:\n",
    "            if word not in self.model.wv:\n",
    "                return None\n",
    "            \n",
    "            # Clear cache if too large\n",
    "            if len(self._word_cache) >= self._cache_size:\n",
    "                remove_count = self._cache_size // 5\n",
    "                for _ in range(remove_count):\n",
    "                    self._word_cache.popitem()\n",
    "            \n",
    "            self._word_cache[word] = self.model.wv[word]\n",
    "        \n",
    "        return self._word_cache[word]\n",
    "    \n",
    "    def show_vocabulary(self, limit=50, start_from=0):\n",
    "        \"\"\"Display vocabulary words with indices.\"\"\"\n",
    "        print(f\"\\nüìö Vocabulary (showing {limit} words starting from {start_from}):\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        vocab = self.model.wv.index_to_key\n",
    "        end = min(start_from + limit, len(vocab))\n",
    "        \n",
    "        for i in range(start_from, end):\n",
    "            word = vocab[i]\n",
    "            print(f\"  {i:4d}. {word}\")\n",
    "        \n",
    "        print(f\"\\nüìä Total vocabulary: {len(vocab):,} words\")\n",
    "        \n",
    "        if end < len(vocab):\n",
    "            print(f\"üí° Use show_vocabulary(limit={limit}, start_from={end}) to see more\")\n",
    "    \n",
    "    def search_vocabulary(self, pattern, max_results=50):\n",
    "        \"\"\"Search vocabulary by pattern.\"\"\"\n",
    "        print(f\"\\nüîç Searching for words matching '{pattern}':\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        vocab = self.model.wv.index_to_key\n",
    "        matches = [w for w in vocab if pattern.lower() in w.lower()]\n",
    "        \n",
    "        if not matches:\n",
    "            print(f\"‚ùå No matches found for '{pattern}'\")\n",
    "            return\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(matches)} matches (showing first {max_results}):\\n\")\n",
    "        for i, word in enumerate(matches[:max_results], 1):\n",
    "            print(f\"  {i:3d}. {word}\")\n",
    "        \n",
    "        if len(matches) > max_results:\n",
    "            print(f\"\\nüí° {len(matches) - max_results} more matches not shown\")\n",
    "    \n",
    "    def get_vocabulary_stats(self):\n",
    "        \"\"\"Get detailed vocabulary statistics.\"\"\"\n",
    "        vocab = self.model.wv.index_to_key\n",
    "        lengths = [len(w) for w in vocab]\n",
    "        \n",
    "        print(f\"\\nüìä Vocabulary Statistics:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Total words: {len(vocab):,}\")\n",
    "        print(f\"  Shortest word: '{min(vocab, key=len)}' ({min(lengths)} chars)\")\n",
    "        print(f\"  Longest word: '{max(vocab, key=len)}' ({max(lengths)} chars)\")\n",
    "        print(f\"  Average length: {np.mean(lengths):.2f} chars\")\n",
    "        print(f\"  Median length: {np.median(lengths):.0f} chars\")\n",
    "        \n",
    "        # Character distribution\n",
    "        first_chars = {}\n",
    "        for word in vocab:\n",
    "            if word:\n",
    "                first = word[0].lower()\n",
    "                first_chars[first] = first_chars.get(first, 0) + 1\n",
    "        \n",
    "        print(f\"\\nüìù Top 10 starting letters:\")\n",
    "        for char, count in sorted(first_chars.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  '{char}': {count} words ({100*count/len(vocab):.1f}%)\")\n",
    "    \n",
    "    def show_word_vector(self, word):\n",
    "        \"\"\"Display the actual vector for a word.\"\"\"\n",
    "        if word not in self.model.wv:\n",
    "            print(f\"‚ùå '{word}' not in vocabulary!\")\n",
    "            return\n",
    "        \n",
    "        vector = self.model.wv[word]\n",
    "        print(f\"\\nüî¢ Vector for '{word}':\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Dimensions: {len(vector)}\")\n",
    "        print(f\"  First 10 values: {vector[:10]}\")\n",
    "        print(f\"  Min: {vector.min():.4f}\")\n",
    "        print(f\"  Max: {vector.max():.4f}\")\n",
    "        print(f\"  Mean: {vector.mean():.4f}\")\n",
    "        print(f\"  Std: {vector.std():.4f}\")\n",
    "    \n",
    "    def evaluate_analogies(self, analogies):\n",
    "        \"\"\"\n",
    "        Evaluate word analogies with detailed metrics.\n",
    "        \n",
    "        Args:\n",
    "            analogies: List of tuples (a, b, c, expected)\n",
    "                      Tests: a - b + c ‚âà expected\n",
    "                      Example: (\"king\", \"man\", \"queen\", \"woman\")\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (metrics dict, results list)\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìê Evaluating {len(analogies)} analogies...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        correct = 0\n",
    "        attempted = 0\n",
    "        skipped = 0\n",
    "        results = []\n",
    "        \n",
    "        for a, b, c, expected in analogies:\n",
    "            missing = [w for w in [a, b, c, expected] if w not in self.model.wv]\n",
    "            if missing:\n",
    "                skipped += 1\n",
    "                results.append({\n",
    "                    'analogy': f\"{a} - {b} + {c} = ?\",\n",
    "                    'expected': expected,\n",
    "                    'predicted': None,\n",
    "                    'correct': False,\n",
    "                    'skipped': True\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            attempted += 1\n",
    "            \n",
    "            try:\n",
    "                similar = self.model.wv.most_similar(positive=[c, a], negative=[b], topn=5)\n",
    "                \n",
    "                predicted = None\n",
    "                for word, score in similar:\n",
    "                    if word not in [a, b, c]:\n",
    "                        predicted = word\n",
    "                        break\n",
    "                \n",
    "                is_correct = (predicted == expected)\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                results.append({\n",
    "                    'analogy': f\"{a} - {b} + {c}\",\n",
    "                    'expected': expected,\n",
    "                    'predicted': predicted,\n",
    "                    'correct': is_correct,\n",
    "                    'skipped': False,\n",
    "                    'candidates': similar[:3]\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                results.append({\n",
    "                    'analogy': f\"{a} - {b} + {c}\",\n",
    "                    'expected': expected,\n",
    "                    'predicted': None,\n",
    "                    'correct': False,\n",
    "                    'skipped': True\n",
    "                })\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = correct\n",
    "        fp = attempted - correct\n",
    "        fn = attempted - correct\n",
    "        \n",
    "        metrics = calculate_metrics(tp, fp, fn)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä Analogy Evaluation Results:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Total analogies: {len(analogies)}\")\n",
    "        print(f\"  Attempted: {attempted}\")\n",
    "        print(f\"  Skipped (missing words): {skipped}\")\n",
    "        print(f\"  Correct: {correct}\")\n",
    "        print(f\"  Incorrect: {attempted - correct}\")\n",
    "        print(f\"\\n  Accuracy: {(correct/attempted*100) if attempted > 0 else 0:.2f}%\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìù Sample Results (first 10):\")\n",
    "        print(\"=\"*60)\n",
    "        for i, result in enumerate(results[:10], 1):\n",
    "            if result['skipped']:\n",
    "                print(f\"  {i}. {result['analogy']} = {result['expected']} ‚ö†Ô∏è SKIPPED\")\n",
    "            else:\n",
    "                status = \"‚úÖ\" if result['correct'] else \"‚ùå\"\n",
    "                print(f\"  {i}. {result['analogy']} = {result['expected']}\")\n",
    "                print(f\"      Predicted: {result['predicted']} {status}\")\n",
    "        \n",
    "        return metrics, results\n",
    "    \n",
    "    def evaluate_similarity(self, word_pairs):\n",
    "        \"\"\"\n",
    "        Evaluate word similarity against human judgments.\n",
    "        \n",
    "        Args:\n",
    "            word_pairs: List of tuples (word1, word2, human_score)\n",
    "                       human_score: 0-1 or 0-10 (normalized automatically)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with correlation metrics and scores\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìè Evaluating {len(word_pairs)} word pairs...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        model_scores = []\n",
    "        human_scores = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for word1, word2, human_score in word_pairs:\n",
    "            if word1 not in self.model.wv or word2 not in self.model.wv:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            model_sim = self.model.wv.similarity(word1, word2)\n",
    "            \n",
    "            # Normalize human score to 0-1 if needed\n",
    "            if human_score > 1:\n",
    "                human_score = human_score / 10.0\n",
    "            \n",
    "            model_scores.append(model_sim)\n",
    "            human_scores.append(human_score)\n",
    "        \n",
    "        if len(model_scores) == 0:\n",
    "            print(\"‚ùå No valid word pairs to evaluate!\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate correlation\n",
    "        spearman_corr, spearman_p = spearmanr(human_scores, model_scores)\n",
    "        pearson_corr, pearson_p = pearsonr(human_scores, model_scores)\n",
    "        \n",
    "        # Binary classification metrics (threshold at 0.5)\n",
    "        threshold = 0.5\n",
    "        tp = sum(1 for h, m in zip(human_scores, model_scores) if h >= threshold and m >= threshold)\n",
    "        fp = sum(1 for h, m in zip(human_scores, model_scores) if h < threshold and m >= threshold)\n",
    "        fn = sum(1 for h, m in zip(human_scores, model_scores) if h >= threshold and m < threshold)\n",
    "        tn = sum(1 for h, m in zip(human_scores, model_scores) if h < threshold and m < threshold)\n",
    "        \n",
    "        metrics = calculate_metrics(tp, fp, fn, tn)\n",
    "        \n",
    "        print(f\"\\nüìä Similarity Evaluation Results:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Total pairs: {len(word_pairs)}\")\n",
    "        print(f\"  Evaluated: {len(model_scores)}\")\n",
    "        print(f\"  Skipped: {skipped}\")\n",
    "        print(f\"\\n  Spearman Correlation: {spearman_corr:.4f} (p={spearman_p:.4f})\")\n",
    "        print(f\"  Pearson Correlation: {pearson_corr:.4f} (p={pearson_p:.4f})\")\n",
    "        print(f\"\\n  Binary Classification (threshold={threshold}):\")\n",
    "        print(f\"    Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"    F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'spearman': spearman_corr,\n",
    "            'pearson': pearson_corr,\n",
    "            'metrics': metrics,\n",
    "            'model_scores': model_scores,\n",
    "            'human_scores': human_scores\n",
    "        }\n",
    "    \n",
    "    def evaluate_categorization(self, word_categories):\n",
    "        \"\"\"\n",
    "        Evaluate word categorization task.\n",
    "        \n",
    "        Args:\n",
    "            word_categories: Dict of category_name -> list of words\n",
    "        \n",
    "        Returns:\n",
    "            Metrics dictionary\n",
    "        \"\"\"\n",
    "        print(f\"\\nüè∑Ô∏è Evaluating categorization for {len(word_categories)} categories...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        all_words = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for category, words in word_categories.items():\n",
    "            for word in words:\n",
    "                if word in self.model.wv:\n",
    "                    all_words.append(word)\n",
    "                    true_labels.append(category)\n",
    "        \n",
    "        if len(all_words) < 2:\n",
    "            print(\"‚ùå Not enough valid words for categorization!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Valid words: {len(all_words)}\")\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, word in enumerate(all_words):\n",
    "            true_category = true_labels[i]\n",
    "            similar = self.model.wv.most_similar(word, topn=len(all_words))\n",
    "            \n",
    "            for similar_word, score in similar:\n",
    "                if similar_word in all_words and similar_word != word:\n",
    "                    predicted_category = true_labels[all_words.index(similar_word)]\n",
    "                    if predicted_category == true_category:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    break\n",
    "        \n",
    "        tp = correct\n",
    "        fp = total - correct\n",
    "        fn = total - correct\n",
    "        \n",
    "        metrics = calculate_metrics(tp, fp, fn)\n",
    "        \n",
    "        print(f\"\\nüìä Categorization Results:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Accuracy: {(correct/total*100) if total > 0 else 0:.2f}%\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def interactive_analogy_test(self):\n",
    "        \"\"\"Interactive analogy testing with user input.\"\"\"\n",
    "        print(\"\\nüìê Interactive Analogy Test\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Format: word1 - word2 + word3 = ?\")\n",
    "        print(\"Example: king - man + woman = queen\")\n",
    "        print(\"Enter analogies one per line (empty line to finish)\")\n",
    "        print()\n",
    "        \n",
    "        analogies = []\n",
    "        while True:\n",
    "            line = input(\"  Analogy (format: a b c expected): \").strip()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) == 4:\n",
    "                analogies.append(tuple(parts))\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Invalid format! Use: word1 word2 word3 expected\")\n",
    "        \n",
    "        if analogies:\n",
    "            self.evaluate_analogies(analogies)\n",
    "        else:\n",
    "            print(\"‚ùå No analogies entered!\")\n",
    "    \n",
    "    def interactive_similarity_test(self):\n",
    "        \"\"\"Interactive similarity testing with user input.\"\"\"\n",
    "        print(\"\\nüìè Interactive Similarity Test\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Enter word pairs with human similarity scores\")\n",
    "        print(\"Format: word1 word2 score (score: 0-10 or 0-1)\")\n",
    "        print(\"Example: cat dog 8\")\n",
    "        print(\"Enter pairs one per line (empty line to finish)\")\n",
    "        print()\n",
    "        \n",
    "        word_pairs = []\n",
    "        while True:\n",
    "            line = input(\"  Pair: \").strip()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    word1, word2, score = parts[0], parts[1], float(parts[2])\n",
    "                    word_pairs.append((word1, word2, score))\n",
    "                except ValueError:\n",
    "                    print(\"  ‚ö†Ô∏è  Invalid score! Must be a number\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Invalid format! Use: word1 word2 score\")\n",
    "        \n",
    "        if word_pairs:\n",
    "            self.evaluate_similarity(word_pairs)\n",
    "        else:\n",
    "            print(\"‚ùå No word pairs entered!\")\n",
    "    \n",
    "    def interactive_categorization_test(self):\n",
    "        \"\"\"Interactive categorization testing with user input.\"\"\"\n",
    "        print(\"\\nüè∑Ô∏è Interactive Categorization Test\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Enter categories and their words\")\n",
    "        print(\"Format: category: word1 word2 word3\")\n",
    "        print(\"Example: animals: cat dog bird\")\n",
    "        print(\"Enter categories one per line (empty line to finish)\")\n",
    "        print()\n",
    "        \n",
    "        categories = {}\n",
    "        while True:\n",
    "            line = input(\"  Category: \").strip()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            if ':' in line:\n",
    "                cat_name, words = line.split(':', 1)\n",
    "                cat_name = cat_name.strip()\n",
    "                word_list = words.strip().split()\n",
    "                if word_list:\n",
    "                    categories[cat_name] = word_list\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Invalid format! Use: category: word1 word2 word3\")\n",
    "        \n",
    "        if categories:\n",
    "            self.evaluate_categorization(categories)\n",
    "        else:\n",
    "            print(\"‚ùå No categories entered!\")\n",
    "    \n",
    "    def visualize_vocabulary_2d(self, words=None, num_words=50, highlight_words=None):\n",
    "        \"\"\"Visualize vocabulary in 2D space using FAST PCA.\"\"\"\n",
    "        if not HAS_MATPLOTLIB:\n",
    "            print(\"‚ùå Matplotlib not installed! Cannot create visualization.\")\n",
    "            print(\"üí° Install with: pip install matplotlib\")\n",
    "            return\n",
    "        \n",
    "        if num_words > 200:\n",
    "            print(f\"‚ö†Ô∏è  Limiting to 200 words for speed (you requested {num_words})\")\n",
    "            num_words = 200\n",
    "        \n",
    "        print(f\"\\nüé® Generating 2D visualization using FAST PCA...\")\n",
    "        \n",
    "        if words is None:\n",
    "            words = self.model.wv.index_to_key[:num_words]\n",
    "        else:\n",
    "            words = [w for w in words if w in self.model.wv]\n",
    "            if not words:\n",
    "                print(\"‚ùå None of the specified words are in vocabulary!\")\n",
    "                return\n",
    "            if len(words) > 200:\n",
    "                print(f\"‚ö†Ô∏è  Limiting to 200 words for speed\")\n",
    "                words = words[:200]\n",
    "        \n",
    "        print(f\"   Processing {len(words)} words...\")\n",
    "        \n",
    "        try:\n",
    "            word_vectors = np.array([self.model.wv[word] for word in words])\n",
    "            coords = manual_pca_2d(word_vectors)\n",
    "            \n",
    "            print(f\"   Creating plot...\")\n",
    "            \n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            if highlight_words:\n",
    "                colors = ['red' if w in highlight_words else 'blue' for w in words]\n",
    "                sizes = [100 if w in highlight_words else 50 for w in words]\n",
    "            else:\n",
    "                colors = 'blue'\n",
    "                sizes = 50\n",
    "            \n",
    "            plt.scatter(coords[:, 0], coords[:, 1], c=colors, alpha=0.6, s=sizes)\n",
    "            \n",
    "            label_step = max(1, len(words) // 100)\n",
    "            for i in range(0, len(words), label_step):\n",
    "                word = words[i]\n",
    "                fontsize = 10 if (highlight_words and word in highlight_words) else 8\n",
    "                fontweight = 'bold' if (highlight_words and word in highlight_words) else 'normal'\n",
    "                plt.annotate(word, xy=(coords[i, 0], coords[i, 1]), \n",
    "                            xytext=(3, 3), textcoords='offset points',\n",
    "                            fontsize=fontsize, alpha=0.8, fontweight=fontweight)\n",
    "            \n",
    "            plt.title(f'Word2Vec Vocabulary (FAST PCA) - {len(words)} words', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Principal Component 1')\n",
    "            plt.ylabel('Principal Component 2')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            filename = f'vocab_visualization_pca_{len(words)}words.png'\n",
    "            plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Saved to {filename}\")\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating visualization: {e}\")\n",
    "    \n",
    "    def visualize_word_neighbors(self, word, topn=20):\n",
    "        \"\"\"Visualize a word and its nearest neighbors in 2D space.\"\"\"\n",
    "        if not HAS_MATPLOTLIB:\n",
    "            print(\"‚ùå Matplotlib not installed! Showing text version instead...\\n\")\n",
    "            if word not in self.model.wv:\n",
    "                print(f\"‚ùå '{word}' not in vocabulary!\")\n",
    "                return\n",
    "            \n",
    "            similar = self.model.wv.most_similar(word, topn=topn)\n",
    "            print(f\"üìù '{word}' and its {topn} nearest neighbors:\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"  TARGET: {word}\")\n",
    "            for i, (w, score) in enumerate(similar, 1):\n",
    "                bar_length = int(score * 40)\n",
    "                print(f\"  {i:2d}. {w:15s} [{('‚ñà' * bar_length):40s}] {score:.4f}\")\n",
    "            return\n",
    "        \n",
    "        if word not in self.model.wv:\n",
    "            print(f\"‚ùå '{word}' not in vocabulary!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüéØ Visualizing '{word}' and its {topn} nearest neighbors...\")\n",
    "        \n",
    "        try:\n",
    "            similar = self.model.wv.most_similar(word, topn=topn)\n",
    "            neighbor_words = [w for w, _ in similar]\n",
    "            all_words = [word] + neighbor_words\n",
    "            \n",
    "            self.visualize_vocabulary_2d(\n",
    "                words=all_words,\n",
    "                highlight_words=[word]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nüìä Similarity scores for '{word}':\")\n",
    "            for w, score in similar[:10]:\n",
    "                print(f\"  ‚Ä¢ {w}: {score:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error visualizing neighbors: {e}\")\n",
    "    \n",
    "    def visualize_word_clusters(self, word_groups):\n",
    "        \"\"\"Visualize multiple groups of related words with different colors.\"\"\"\n",
    "        if not HAS_MATPLOTLIB:\n",
    "            print(\"‚ùå Matplotlib not installed! Showing text version instead...\\n\")\n",
    "            if isinstance(word_groups, dict):\n",
    "                for group_name, words in word_groups.items():\n",
    "                    valid_words = [w for w in words if w in self.model.wv]\n",
    "                    print(f\"\\nüì¶ Group: {group_name} ({len(valid_words)} words)\")\n",
    "                    print(\"=\"*60)\n",
    "                    for word in valid_words:\n",
    "                        print(f\"  ‚Ä¢ {word}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüé® Visualizing word clusters...\")\n",
    "        \n",
    "        try:\n",
    "            if isinstance(word_groups, dict):\n",
    "                all_words = []\n",
    "                colors = []\n",
    "                color_map = plt.cm.get_cmap('tab10')\n",
    "                group_info = []\n",
    "                \n",
    "                for i, (group_name, words) in enumerate(word_groups.items()):\n",
    "                    valid_words = [w for w in words if w in self.model.wv]\n",
    "                    all_words.extend(valid_words)\n",
    "                    colors.extend([color_map(i)] * len(valid_words))\n",
    "                    group_info.append((group_name, len(valid_words), color_map(i)))\n",
    "                    print(f\"  Group '{group_name}': {len(valid_words)} words\")\n",
    "            else:\n",
    "                all_words = []\n",
    "                colors = []\n",
    "                color_map = plt.cm.get_cmap('tab10')\n",
    "                group_info = []\n",
    "                \n",
    "                for i, words in enumerate(word_groups):\n",
    "                    valid_words = [w for w in words if w in self.model.wv]\n",
    "                    all_words.extend(valid_words)\n",
    "                    colors.extend([color_map(i)] * len(valid_words))\n",
    "            \n",
    "            if not all_words:\n",
    "                print(\"‚ùå No valid words found!\")\n",
    "                return\n",
    "            \n",
    "            if len(all_words) > 200:\n",
    "                print(f\"‚ö†Ô∏è  Limiting to 200 words for speed\")\n",
    "                all_words = all_words[:200]\n",
    "                colors = colors[:200]\n",
    "            \n",
    "            print(f\"   Processing {len(all_words)} words...\")\n",
    "            \n",
    "            word_vectors = np.array([self.model.wv[word] for word in all_words])\n",
    "            coords = manual_pca_2d(word_vectors)\n",
    "            \n",
    "            print(f\"   Creating plot...\")\n",
    "            \n",
    "            plt.figure(figsize=(14, 10))\n",
    "            plt.scatter(coords[:, 0], coords[:, 1], c=colors, alpha=0.6, s=100)\n",
    "            \n",
    "            label_step = max(1, len(all_words) // 80)\n",
    "            for i in range(0, len(all_words), label_step):\n",
    "                word = all_words[i]\n",
    "                plt.annotate(word, xy=(coords[i, 0], coords[i, 1]),\n",
    "                            xytext=(3, 3), textcoords='offset points',\n",
    "                            fontsize=9, alpha=0.8, fontweight='bold')\n",
    "            \n",
    "            if isinstance(word_groups, dict) and group_info:\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=f'{name} ({count})')\n",
    "                       for name, count, color in group_info]\n",
    "    plt.legend(handles=legend_elements, loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
